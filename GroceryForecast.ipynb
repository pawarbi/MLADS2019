{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practicalities of AutoML forecasting\n",
    "\n",
    "In this notebook, we will walk through the process of trying to actually push a real world dataset through AutoML. AutoML has expectations of clean data which are rarely encountered with real world data. We will show various techniques of detecting the problems before they crash AutoML, and workarounds that will get you through the day.\n",
    "\n",
    "Eventually, these tips and tricks will become part of the AutoML product, prettied up and made more robust.\n",
    "\n",
    "Many of the cells in this notebook will show the first attempt at doing something, which is later abandoned or cleaned up. They are marked with `DEAD END`. Don't run them, they are there so that you can learn from my mistakes, not your.\n",
    "\n",
    "To begin, let's load some data. Our first dead end occurs in the data loading, and is shown in the companion notebook `GetDataFromSQL`. But for now, _assume_ we have the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SalesDate</th>\n",
       "      <th>Item</th>\n",
       "      <th>Site</th>\n",
       "      <th>Channel</th>\n",
       "      <th>Quantity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1262885</td>\n",
       "      <td>1262885</td>\n",
       "      <td>1262885</td>\n",
       "      <td>1262885</td>\n",
       "      <td>1262885.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2057</td>\n",
       "      <td>348</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>2017-05-26</td>\n",
       "      <td>A53634</td>\n",
       "      <td>82DC</td>\n",
       "      <td>811B</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>996</td>\n",
       "      <td>43416</td>\n",
       "      <td>359820</td>\n",
       "      <td>391703</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7928.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         SalesDate     Item     Site  Channel   Quantity\n",
       "count      1262885  1262885  1262885  1262885 1262885.00\n",
       "unique        2057      348       11        9        nan\n",
       "top     2017-05-26   A53634     82DC     811B        nan\n",
       "freq           996    43416   359820   391703        nan\n",
       "mean           NaN      NaN      NaN      NaN      12.02\n",
       "std            NaN      NaN      NaN      NaN      25.88\n",
       "min            NaN      NaN      NaN      NaN       1.00\n",
       "25%            NaN      NaN      NaN      NaN       5.00\n",
       "50%            NaN      NaN      NaN      NaN       8.00\n",
       "75%            NaN      NaN      NaN      NaN      12.00\n",
       "max            NaN      NaN      NaN      NaN    7928.00"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "SQLdata = pandas.read_csv('data/tutorial_data.csv.bz2')\n",
    "SQLdata.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is a friendly 1.2M rows, which is because it only contains the 348 items beginning with 'A'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQLdata['Item'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll establish the time series metadata. These say: we want to forecast the Quantity, along the time axis given by SalesDate, for each combination of Item, Site, and Channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grain_colnames = ['Item', 'Site', 'Channel']\n",
    "time_colname = 'SalesDate'\n",
    "target_colname = 'Quantity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many series do we have?\n",
    "guppy = SQLdata.groupby(grain_colnames)\n",
    "\n",
    "print(\"Rows pulled : \" + str(len(SQLdata)))\n",
    "print(\"Distinct time series : \" + str(len(guppy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's forecast!\n",
    "\n",
    "AutoML in theory does not need much more than a data frame. So let's see how far AutoML gets on this **tough** dataset. Let us not even split into train and test (because we may need to re-work the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = SQLdata.copy()\n",
    "y_train = X_train.pop(target_colname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin our forecasting journey with the boilerplate AutoML imports and connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import warnings\n",
    "import os\n",
    "# Squash warning messages for cleaner output in the notebook\n",
    "warnings.showwarning = lambda *args, **kwargs: None\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-f5af4d07f285>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mws\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWorkspace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# choose a name for the run history container in the workspace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mexperiment_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'automl-grocery'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# project folder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\azure_automl\\lib\\site-packages\\azureml\\core\\workspace.py\u001b[0m in \u001b[0;36mfrom_config\u001b[1;34m(path, auth, _logger, _file_name)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[0m_logger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Found the config file in: %s'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfound_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mWorkspace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubscription_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresource_group\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkspace_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\azure_automl\\lib\\site-packages\\azureml\\core\\workspace.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, subscription_id, resource_group, workspace_name, auth, _location, _disable_service_check)\u001b[0m\n\u001b[0;32m     88\u001b[0m         \"\"\"\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mauth\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m             \u001b[0mauth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInteractiveLoginAuthentication\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_auth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mauth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\azure_automl\\lib\\site-packages\\azureml\\core\\authentication.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, force, tenant_id)\u001b[0m\n\u001b[0;32m    326\u001b[0m                     print(\"Performing interactive authentication. Please follow the instructions \"\n\u001b[0;32m    327\u001b[0m                           \"on the terminal.\")\n\u001b[1;32m--> 328\u001b[1;33m                     \u001b[0mperform_interactive_login\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtenant\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtenant_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    329\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Interactive authentication successfully completed.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\azure_automl\\lib\\site-packages\\azureml\\_base_sdk_common\\common.py\u001b[0m in \u001b[0;36mperform_interactive_login\u001b[1;34m(username, password, service_principal, tenant, allow_no_subscriptions, identity, use_device_code, use_cert_sn_issuer)\u001b[0m\n\u001b[0;32m    579\u001b[0m             \u001b[0muse_device_code\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_device_code\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m             \u001b[0mallow_no_subscriptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mallow_no_subscriptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m             use_cert_sn_issuer=use_cert_sn_issuer)\n\u001b[0m\u001b[0;32m    582\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mAdalError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m         \u001b[1;31m# try polish unfriendly server errors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\azure_automl\\lib\\site-packages\\azureml\\_vendor\\azure_cli_core\\_profile.py\u001b[0m in \u001b[0;36mfind_subscriptions_on_login\u001b[1;34m(self, interactive, username, password, is_service_principal, tenant, use_device_code, allow_no_subscriptions, subscription_finder, use_cert_sn_issuer)\u001b[0m\n\u001b[0;32m    217\u001b[0m                     \u001b[0mauthority_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_authority_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcloud_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtenant\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m                     subscriptions = subscription_finder.find_through_authorization_code_flow(\n\u001b[1;32m--> 219\u001b[1;33m                         tenant, self._ad_resource_uri, authority_url)\n\u001b[0m\u001b[0;32m    220\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m                     \u001b[0muse_device_code\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\azure_automl\\lib\\site-packages\\azureml\\_vendor\\azure_cli_core\\_profile.py\u001b[0m in \u001b[0;36mfind_through_authorization_code_flow\u001b[1;34m(self, tenant, resource, authority_url)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m         \u001b[1;31m# launch browser and get the code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 764\u001b[1;33m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_authorization_code\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauthority_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    765\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'code'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\azure_automl\\lib\\site-packages\\azureml\\_vendor\\azure_cli_core\\_profile.py\u001b[0m in \u001b[0;36m_get_authorization_code\u001b[1;34m(resource, authority_url)\u001b[0m\n\u001b[0;32m   1111\u001b[0m     \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1113\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# so that ctrl+c can stop the command\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1114\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1115\u001b[0m             \u001b[1;32mbreak\u001b[0m  \u001b[1;31m# done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "# choose a name for the experiment in the workspace\n",
    "# 'Experiment' roughly corresponds to a 'problem to solve'\n",
    "experiment_name = 'automl-grocery'\n",
    "# where to write models and other artifacts locally\n",
    "project_folder = './sample_projects/automl-grocery'\n",
    "experiment = Experiment(ws, experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {}\n",
    "output['SDK version'] = azureml.core.VERSION\n",
    "output['Subscription ID'] = ws.subscription_id\n",
    "output['Workspace'] = ws.name\n",
    "output['Resource Group'] = ws.resource_group\n",
    "output['Location'] = ws.location\n",
    "output['Project Directory'] = project_folder\n",
    "output['Run History Name'] = experiment_name\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "outputDf = pd.DataFrame(data = output, index = [''])\n",
    "outputDf.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we specify what our forecasting problem is, put the spec in the config object form, and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoMLConfig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-c9ca9af65a8f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m }\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m automl_config = AutoMLConfig(task='forecasting',\n\u001b[0m\u001b[0;32m      9\u001b[0m                              \u001b[0mdebug_log\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'automl-grocery.log'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                              \u001b[0mprimary_metric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'normalized_root_mean_squared_error'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'AutoMLConfig' is not defined"
     ]
    }
   ],
   "source": [
    "time_series_settings = {\n",
    "    'time_column_name': time_colname,\n",
    "    'grain_column_names': grain_colnames,\n",
    "    'drop_column_names': [],\n",
    "    'max_horizon': 31\n",
    "}\n",
    "\n",
    "automl_config = AutoMLConfig(task='forecasting',\n",
    "                             debug_log='automl-grocery.log',\n",
    "                             primary_metric='normalized_root_mean_squared_error',\n",
    "                             iterations=5,\n",
    "                             X=X_train,\n",
    "                             y=y_train,                             \n",
    "                             n_cross_validations=2,\n",
    "                             enable_ensembling=False,\n",
    "                             path=project_folder,\n",
    "                             verbosity=logging.INFO,    \n",
    "                             **time_series_settings)\n",
    "\n",
    "local_run = experiment.submit(automl_config, show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The errors are expected: our dataset has many intermittent series. For example, look at this series, focusing on the `SalesDate` column. Could *you* determine what its natural reporting frequency is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SalesDate</th>\n",
       "      <th>Item</th>\n",
       "      <th>Site</th>\n",
       "      <th>Channel</th>\n",
       "      <th>Quantity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>79490</th>\n",
       "      <td>2012-01-24</td>\n",
       "      <td>A0CA5B</td>\n",
       "      <td>1B3B</td>\n",
       "      <td>811B</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79491</th>\n",
       "      <td>2012-01-30</td>\n",
       "      <td>A0CA5B</td>\n",
       "      <td>1B3B</td>\n",
       "      <td>811B</td>\n",
       "      <td>9.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79492</th>\n",
       "      <td>2012-02-02</td>\n",
       "      <td>A0CA5B</td>\n",
       "      <td>1B3B</td>\n",
       "      <td>811B</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79493</th>\n",
       "      <td>2012-02-04</td>\n",
       "      <td>A0CA5B</td>\n",
       "      <td>1B3B</td>\n",
       "      <td>811B</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79494</th>\n",
       "      <td>2012-02-17</td>\n",
       "      <td>A0CA5B</td>\n",
       "      <td>1B3B</td>\n",
       "      <td>811B</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79495</th>\n",
       "      <td>2012-02-29</td>\n",
       "      <td>A0CA5B</td>\n",
       "      <td>1B3B</td>\n",
       "      <td>811B</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79496</th>\n",
       "      <td>2012-03-07</td>\n",
       "      <td>A0CA5B</td>\n",
       "      <td>1B3B</td>\n",
       "      <td>811B</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79497</th>\n",
       "      <td>2012-03-10</td>\n",
       "      <td>A0CA5B</td>\n",
       "      <td>1B3B</td>\n",
       "      <td>811B</td>\n",
       "      <td>7.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79498</th>\n",
       "      <td>2012-03-14</td>\n",
       "      <td>A0CA5B</td>\n",
       "      <td>1B3B</td>\n",
       "      <td>811B</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79499</th>\n",
       "      <td>2012-03-23</td>\n",
       "      <td>A0CA5B</td>\n",
       "      <td>1B3B</td>\n",
       "      <td>811B</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79500</th>\n",
       "      <td>2012-03-24</td>\n",
       "      <td>A0CA5B</td>\n",
       "      <td>1B3B</td>\n",
       "      <td>811B</td>\n",
       "      <td>9.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79501</th>\n",
       "      <td>2012-03-26</td>\n",
       "      <td>A0CA5B</td>\n",
       "      <td>1B3B</td>\n",
       "      <td>811B</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79502</th>\n",
       "      <td>2012-04-25</td>\n",
       "      <td>A0CA5B</td>\n",
       "      <td>1B3B</td>\n",
       "      <td>811B</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79503</th>\n",
       "      <td>2012-05-14</td>\n",
       "      <td>A0CA5B</td>\n",
       "      <td>1B3B</td>\n",
       "      <td>811B</td>\n",
       "      <td>8.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79504</th>\n",
       "      <td>2012-05-17</td>\n",
       "      <td>A0CA5B</td>\n",
       "      <td>1B3B</td>\n",
       "      <td>811B</td>\n",
       "      <td>8.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79505</th>\n",
       "      <td>2012-05-24</td>\n",
       "      <td>A0CA5B</td>\n",
       "      <td>1B3B</td>\n",
       "      <td>811B</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79506</th>\n",
       "      <td>2012-06-14</td>\n",
       "      <td>A0CA5B</td>\n",
       "      <td>1B3B</td>\n",
       "      <td>811B</td>\n",
       "      <td>7.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79507</th>\n",
       "      <td>2012-06-16</td>\n",
       "      <td>A0CA5B</td>\n",
       "      <td>1B3B</td>\n",
       "      <td>811B</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79508</th>\n",
       "      <td>2012-06-25</td>\n",
       "      <td>A0CA5B</td>\n",
       "      <td>1B3B</td>\n",
       "      <td>811B</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79509</th>\n",
       "      <td>2012-06-28</td>\n",
       "      <td>A0CA5B</td>\n",
       "      <td>1B3B</td>\n",
       "      <td>811B</td>\n",
       "      <td>8.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79510</th>\n",
       "      <td>2012-07-20</td>\n",
       "      <td>A0CA5B</td>\n",
       "      <td>1B3B</td>\n",
       "      <td>811B</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        SalesDate    Item  Site Channel  Quantity\n",
       "79490  2012-01-24  A0CA5B  1B3B    811B      4.00\n",
       "79491  2012-01-30  A0CA5B  1B3B    811B      9.00\n",
       "79492  2012-02-02  A0CA5B  1B3B    811B      4.00\n",
       "79493  2012-02-04  A0CA5B  1B3B    811B      3.00\n",
       "79494  2012-02-17  A0CA5B  1B3B    811B      5.00\n",
       "79495  2012-02-29  A0CA5B  1B3B    811B      4.00\n",
       "79496  2012-03-07  A0CA5B  1B3B    811B      4.00\n",
       "79497  2012-03-10  A0CA5B  1B3B    811B      7.00\n",
       "79498  2012-03-14  A0CA5B  1B3B    811B      5.00\n",
       "79499  2012-03-23  A0CA5B  1B3B    811B      5.00\n",
       "79500  2012-03-24  A0CA5B  1B3B    811B      9.00\n",
       "79501  2012-03-26  A0CA5B  1B3B    811B      5.00\n",
       "79502  2012-04-25  A0CA5B  1B3B    811B     10.00\n",
       "79503  2012-05-14  A0CA5B  1B3B    811B      8.00\n",
       "79504  2012-05-17  A0CA5B  1B3B    811B      8.00\n",
       "79505  2012-05-24  A0CA5B  1B3B    811B      6.00\n",
       "79506  2012-06-14  A0CA5B  1B3B    811B      7.00\n",
       "79507  2012-06-16  A0CA5B  1B3B    811B      6.00\n",
       "79508  2012-06-25  A0CA5B  1B3B    811B      4.00\n",
       "79509  2012-06-28  A0CA5B  1B3B    811B      8.00\n",
       "79510  2012-07-20  A0CA5B  1B3B    811B     10.00"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SQLdata[(SQLdata['Item'] == 'A0CA5B') & (SQLdata['Site'] == '1B3B') & (SQLdata['Channel'] == '811B')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technique 1: Filling in the implicit zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's help AutoML determine what the periodicity is by making the series explicitly daily. Since we are talking about sales, missing values correspond to zero quantity sold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have a nice helpful function that fills out the data frame with:\n",
    "# * zeros for the target column\n",
    "# * NaNs for the rest of the data\n",
    "# \n",
    "# Let's import it from out time series library in the repo.\n",
    "from tslib.ts_functions import fill_out_with_zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since frequency is hard to infer with intermittent data, we will tell the function explicitly\n",
    "# the frequency is daily\n",
    "complete = fill_out_with_zeros(SQLdata, time_colname, grain_colnames, target_colname, 'D')\n",
    "complete.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the data engorged to 6M rows - all the days with no record now have explicit 0 sales. We call that a \"flat\" dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data comes out with the grain and time in a pandas MultiIndex\n",
    "flat_complete = complete.reset_index()\n",
    "flat_complete.to_csv(\"grocery_flat.csv.bz2\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how do I split the data? It's about 5 years of data, 2012-01-01 to 2017-08-31.\n",
    "# It probably does not make sense to do more than one month ahead of daily predictions\n",
    "n_test_periods = 31 # days in August 2017\n",
    "\n",
    "# this will split off the last n VALUES from each grain. If the data has implicit zeros,\n",
    "# that means the test TIME INTERVAL may vary by grain.\n",
    "from tslib.ts_functions import split_last_n_by_grain\n",
    "\n",
    "# Do note the use of the parameter specifying the minimum grain length.\n",
    "# Because of how we pulled the data from SQL, the use is redundant.\n",
    "# But if your whole data fits in memory, you can filter out the short grains\n",
    "# in python, rather than at the source, using the splitting function\n",
    "df_train, df_test = split_last_n_by_grain(complete, \n",
    "                                          n_test_periods, \n",
    "                                          time_colname, grain_colnames, \n",
    "                                          min_grain_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know how many datapoints we have. But do we really know where the train and test sets are situated in time?\n",
    "Let's look at their time extents. For ease of plotting, only two items (but with all their Site and Channel combinations) will be shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller = complete.loc[(['A00968', 'A09D61'], slice(None), slice(None), slice(None)), :]\n",
    "small_train, small_test = split_last_n_by_grain(smaller, n_test_periods, time_colname, grain_colnames, min_grain_length=100)\n",
    "print(small_train.shape)\n",
    "print(small_test.shape)\n",
    "\n",
    "# this is for checking the time extent of the grains\n",
    "ranges_train = small_train.reset_index().groupby(grain_colnames).agg({'SalesDate': ['min', 'max']})\n",
    "ranges_train.columns = ['begin', 'end']\n",
    "ranges_train_flat = ranges_train.reset_index()\n",
    "\n",
    "ranges_test = small_test.reset_index().groupby(grain_colnames).agg({'SalesDate': ['min', 'max']})\n",
    "ranges_test.columns = ['begin', 'end']\n",
    "ranges_test_flat = ranges_test.reset_index()\n",
    "\n",
    "# we should be able to plot that\n",
    "%matplotlib notebook\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = [10, 12]\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hlines(range(len(ranges_train_flat)),xmin=ranges_train_flat['begin'].values,xmax=ranges_train_flat['end'].values)\n",
    "plt.hlines(range(len(ranges_test_flat)),xmin=ranges_test_flat['begin'].values,xmax=ranges_test_flat['end'].values, colors='r')\n",
    "ytick_tuples = list(zip(ranges_train_flat['Item'],ranges_train_flat['Site'], ranges_train_flat['Channel']))\n",
    "ytick_labels = list(map(lambda t : '-'.join(t), ytick_tuples))\n",
    "plt.yticks(range(len(ranges_test_flat)), ytick_labels)\n",
    "plt.show()\n",
    "\n",
    "# Improvements on this plot could include \n",
    "#    * plot little series instead of horizontal lines, \n",
    "#    * color-separating zero values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should ask ourselves whether the items whose histories do not reach the temporal end of the dataset are worth forecasting. Those items may have been discontinued, or they are still carried by the store but purchased infrequently. Since that is a question only a business owner can answer, let's leave them in for now. But have that little practicality in mind.\n",
    "\n",
    "Let's save the dataset we have in case we need to restart from this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint: flatten for writing and save\n",
    "flat_test = df_test.reset_index()\n",
    "flat_test.to_csv(\"grocery_flat_test.csv.bz2\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_train = df_train.reset_index()\n",
    "flat_train.to_csv(\"grocery_flat_trainvalid.csv.bz2\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine what X and Y are - now with the zeros\n",
    "X_train = flat_train\n",
    "X_test = flat_test\n",
    "y_train = X_train.pop(target_colname)\n",
    "y_test = X_test.pop(target_colname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_config = AutoMLConfig(task='forecasting',\n",
    "                             debug_log='automl-grocery.log',\n",
    "                             primary_metric='normalized_root_mean_squared_error',\n",
    "                             iterations=5,\n",
    "                             X=X_train,   # Here the X_train and y_train have\n",
    "                             y=y_train,   # explicit zeroes                             \n",
    "                             n_cross_validations=2,\n",
    "                             enable_ensembling=False,\n",
    "                             path=project_folder,\n",
    "                             verbosity=logging.INFO,    \n",
    "                             **time_series_settings)\n",
    "\n",
    "local_run = experiment.submit(automl_config, show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we are not testing here because we pretend AutoML had a memory error. The data we are using in the tutorial may not actually be too large as we are only using items beginning with 'A'. But with the full dataset, the AutoML run in the above cell did fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technique 2: Splitting the datasets and composite models\n",
    "\n",
    "## (NOT your usual train/test splitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint: restarting the processing from saved files\n",
    "# A good practice if you work with dataset sizes that crash python kernels\n",
    "grain_colnames = ['Item', 'Site', 'Channel']\n",
    "time_colname = 'SalesDate'\n",
    "target_colname = 'Quantity'\n",
    "\n",
    "import pandas as pd\n",
    "flat_train = pd.read_csv(\"grocery_flat_trainvalid.csv.bz2\", parse_dates=[time_colname])\n",
    "flat_test = pd.read_csv(\"grocery_flat_test.csv.bz2\", parse_dates=[time_colname])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We arrived at this point because AutoML had an out-of-memory event. \n",
    "\n",
    "If the data is too large, we need to split the series into multiple chunks to be handled separately. The data amounts in the individual chunks should ideally be close to equal.\n",
    "\n",
    "It is best to split by the value of a specified column. It should be one of the grain columns - that way grains are guaranteed not to split across bins. \n",
    "\n",
    "'Item' seems like a good candidate column. How variable do the Item-based groups get in size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_train.groupby('Item').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the size is quite variable. Randomly assigning each item to a subset (chunk) would likely achieve approximately even chunks if the Items were about equal in data size, but we see the variation spans orders of magnitudes.\n",
    "\n",
    "Therefore, after grouping by column value, then approximately solve the bin packing problem with maximum volume equal to the max number of rows we are willing to have for one model. This will guarantee near-optimal packing.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslib.ts_functions import split_into_chunks_by_size, split_into_chunks_by_groups\n",
    "    \n",
    "max_number_of_rows = 1 * 1000 * 1000;\n",
    "train_frames, indices = split_into_chunks_by_size(flat_train, 'Item', max_number_of_rows)\n",
    "# split test set using the same assignment of item to chunk (indices)\n",
    "test_frames = split_into_chunks_by_groups(flat_test, 'Item', indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpointing: pickle the split datasets\n",
    "import pickle\n",
    "pickle.dump( (train_frames, test_frames, indices), open('split_datasets.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and load them back after the lengthy procedure\n",
    "import pickle\n",
    "(train_frames, test_frames, indices) = pickle.load(open('split_datasets.pkl', 'rb'))\n",
    "\n",
    "grain_colnames = ['Item', 'Site', 'Channel']\n",
    "time_colname = 'SalesDate'\n",
    "target_colname = 'Quantity'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ensure we got decently sized chunks. Each chunk is a separate dataset that we will feed into AutoML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "model_valid_for = indices[idx]\n",
    "X_train = train_frames[idx].copy()\n",
    "X_test = test_frames[idx].copy()\n",
    "\n",
    "y_train = X_train.pop(target_colname).values\n",
    "y_test = X_test.pop(target_colname).values\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the first chunk has the expected size for the train and test dataset. Let's test whether we messed up the datasets in the process of splitting. Running it through AutoML will run the data checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_config = AutoMLConfig(task='forecasting',\n",
    "                             debug_log='automl-grocery.log',\n",
    "                             primary_metric='normalized_root_mean_squared_error',\n",
    "                             iterations=5,\n",
    "                             X=X_train,   # set the train to the NEW train set\n",
    "                             y=y_train,   # which is now the first chunk only                      \n",
    "                             n_cross_validations=2,\n",
    "                             enable_ensembling=False,\n",
    "                             path=project_folder,\n",
    "                             verbosity=logging.INFO,    \n",
    "                             **time_series_settings)\n",
    "\n",
    "local_run = experiment.submit(automl_config, show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run, fitted_pipeline = local_run.get_output()\n",
    "fitted_pipeline.steps\n",
    "y_query = y_test.copy()\n",
    "y_query.fill(np.nan)\n",
    "y_fcst, X_trans = fitted_pipeline.forecast(X_test, y_query)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslib.ts_functions import align_outputs\n",
    "df_all = align_outputs(y_fcst, X_trans, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That worked! Now that we were able to get a model, let's import the preferred and customary forecasting metrics and see how well we are doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'MAE'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-602fc44d0e77>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtslib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mts_functions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMAPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSMAPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"[Test Data] \\nRoot Mean squared error: %.2f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mean_absolute_error score: %.2f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'MAPE: %.2f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mMAPE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'MAE'"
     ]
    }
   ],
   "source": [
    "from tslib.ts_functions import MAPE, SMAPE, MAE\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(df_all[target_column_name], df_all['predicted']))\n",
    "print(\"[Test Data] \\nRoot Mean squared error: %.2f\" % rmse)\n",
    "mae = mean_absolute_error(df_all[target_column_name], df_all['predicted'])\n",
    "print('mean_absolute_error score: %.2f' % mae)\n",
    "mape = MAPE(df_all[target_column_name], df_all['predicted'])\n",
    "print('MAPE: %.2f' % mape)\n",
    "smape = SMAPE(df_all[target_column_name], df_all['predicted'])\n",
    "print('SMAPE: %.2f' % smape)\n",
    "mae = MAE(df_all[target_column_name], df_all['predicted'])\n",
    "print('MAE: %.2f' % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MAPE, and even SMAPE metrics can be deceiving when dealing with time series that are often close to zero. They divide by small values and are quite volatile. This makes them a poor fit for such problems. \n",
    "\n",
    "MAE is a better metric for these cases because it avoids division. But be careful because MAE results are unnormalized and comparing them only makes sense in reference to a fixed test set. MAE places weight on series whose absolute value is large. This is often correct in business. Mis-forecasting the Microsoft intern program travel expenses is a lot smaller deal than mis-forecasting the Azure datacenter operations cost. Series trained towards MAE should be of similar magnitude. \n",
    "\n",
    "It is often helpful to stratify the series by the `ceil(log(mean(series)))` and train separate models which allows for more homogeneous training set magnitude-wise and will lead to better performance on smaller-magnitude series.\n",
    "\n",
    "But the best way to get a sense of how well a classifier is performing is to just plot the forecasts vs actuals. The mini-library has some plotting functions for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-d31f077cd21c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m         'Channel' : '811B' }\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m plot_forecast(X_train, y_train, X_test, y_test, y_fcst,\n\u001b[0m\u001b[0;32m     18\u001b[0m                   \u001b[0mtarget_colname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m                   \u001b[0mtime_colname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "# Let's do a time series plot for our difficult intermittent series\n",
    "from tslib.ts_plot_utils import plot_forecast\n",
    "\n",
    "\n",
    "#def plot_forecast(X_trainval, y_trainval,\n",
    "#                  X_test, y_test, y_pred,\n",
    "#                  target_column_name,\n",
    "#                  time_column_name,\n",
    "#                  actual_color='blue',\n",
    "#                  pred_color='green',\n",
    "#                  filter_dict = None)\n",
    "\n",
    "filt = {'Item' : 'A0CA5B',\n",
    "        'Site' : '1B3B',\n",
    "        'Channel' : '811B' }\n",
    "\n",
    "plot_forecast(X_train, y_train, X_test, y_test, y_fcst,\n",
    "                  target_colname,\n",
    "                  time_colname,\n",
    "                  filter_dict = filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do a scatter plot of actual vs predicted\n",
    "\n",
    "%matplotlib notebook\n",
    "test_pred = plt.scatter(df_all[target_column_name], df_all['predicted'], color='b')\n",
    "test_actl = plt.scatter(df_all[target_column_name], df_all[target_column_name], color='g')\n",
    "plt.legend((test_pred, test_actl), ('prediction', 'actual'), loc='upper left', fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting the models back together\n",
    "\n",
    "We split the data into small enough chunks that we can learn a model from each. But this is mildly inconvenient - we now have a collection of models and each of them can only process a subset of the data. We need to keep track of which models serves which `Item`s. To do this without writing awkward code on your side, we offer the general technique of *composite models*.\n",
    "\n",
    "In the forecasting case, we usually have a natural splitting column in the grain. It would be extraordinary to have a single time series (no grain) big enough to require this treatment.\n",
    "\n",
    "In the more general case of classification or regression, no good categorical splitting column may exist in the data. You can still split the data, and all the models you learn will be \"universal\" - capable of classifying any instance. You will lose some predictive power, because you learned from smaller datasets. But you can get it back by running the example through `predict()` of *all* the models and ensembling the predictions (just take a simple majority vote or average).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompositeModel:    \n",
    "    \n",
    "    def __init__(self, split_column, target_column):\n",
    "        self._split_column = split_column\n",
    "        self._target_column = target_column\n",
    "        self._item_run_map = dict()\n",
    "        self._item_model_map = dict()\n",
    "        self._model_impls = dict()\n",
    "        self._indices = dict()\n",
    "    \n",
    "\n",
    "    # todo: redo this so that this class splits its own training data\n",
    "    # on the splitcolumn, ideally in sequence, rather than materializing\n",
    "    # the whole dataset again in memory as a sequence of chunks. \n",
    "    \n",
    "    def fit(self, train_frames, indices):    \n",
    "        \n",
    "        self._indices = indices\n",
    "            \n",
    "        # TODO: this is hard-wired and overwrites outer scope        \n",
    "        time_series_settings = {\n",
    "            'time_column_name': time_colname,\n",
    "            'grain_column_names': grain_colnames,\n",
    "            'drop_column_names': [],\n",
    "            'max_horizon': 31\n",
    "        }\n",
    "        \n",
    "        for idx, Xy in enumerate(train_frames):\n",
    "            \n",
    "            if len(Xy) == 0:\n",
    "                print('Warning: found a zero-length frame at index ' + str(idx))\n",
    "                continue\n",
    "            \n",
    "            X_train = Xy.copy()            \n",
    "            y_train = X_train.pop(self._target_column).values                        \n",
    "          \n",
    "            automl_config = AutoMLConfig(task='forecasting',\n",
    "                                debug_log='automl-grocery.log',\n",
    "                                primary_metric='normalized_root_mean_squared_error',\n",
    "                                iterations=5,\n",
    "                                X=X_train,\n",
    "                                y=y_train,                             \n",
    "                                n_cross_validations=3,\n",
    "                                enable_ensembling=False,\n",
    "                                path=project_folder,\n",
    "                                verbosity=logging.INFO,    \n",
    "                                **time_series_settings)\n",
    "        \n",
    "            # get the model and metadata\n",
    "            local_run = experiment.submit(automl_config, show_output=True) # Parent run \n",
    "            best_run, fitted_pipeline = local_run.get_output()             # Favorite child\n",
    "            model_id = best_run.id\n",
    "            print('Learned model ' + str(model_id))  # this is not working - needs a different ID\n",
    "        \n",
    "            # record the model for item\n",
    "            self._model_impls[model_id] = fitted_pipeline\n",
    "            for idx, item in enumerate(self._indices[idx]):\n",
    "                self._item_model_map[item] = model_id\n",
    "                self._item_run_map[item] = best_run.id\n",
    "                \n",
    "                \n",
    "    def forecast(self, X_test, y_test):\n",
    "        \n",
    "        # split X and y together by splitcolumn\n",
    "        X_copy = X_test.copy()\n",
    "        X_copy['__automl_target_column'] = y_test\n",
    "        chunks = split_into_chunks_by_groups(X_copy, self._split_column, self._indices)\n",
    "        \n",
    "        ys = []\n",
    "        X_transes = []\n",
    "        for chunk in chunks:\n",
    "            \n",
    "            # skip potentially empty splits\n",
    "            if len(chunk) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Look up the right model. It should be the same model \n",
    "            # for the whole chunk by construction\n",
    "            item = chunk.loc[chunk.index[0], self._split_column]\n",
    "            modelid = self._item_model_map[item]\n",
    "            print('Using model ' + str(modelid))\n",
    "            model = self._model_impls[modelid]\n",
    "            \n",
    "            paranoid = True\n",
    "            if paranoid:\n",
    "                for item2 in pd.unique(chunk[self._split_column]):\n",
    "                    assert(\n",
    "                           (item2 in self._item_model_map.keys()) and \n",
    "                           (self._item_model_map[item2] == modelid),\n",
    "                           'Item ' + str(item2) + ' is not mapped to the same model ' + str(modelid) + ' as ' + str(item)\n",
    "                    )\n",
    "                           \n",
    "            y_chunk = chunk.pop('__automl_target_column').values\n",
    "            y_pred, X_trans = model.forecast(chunk, y_chunk)\n",
    "            ys.append(y_chunk)\n",
    "            X_transes.append(X_trans)\n",
    "            \n",
    "        return np.concatenate(ys), pd.concat(X_transes)                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cm = CompositeModel(split_column = 'Item', target_column = 'Quantity')\n",
    "cm.fit(train_frames, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the forecasts on one test dataframe\n",
    "X_test0 = test_frames[0].copy()\n",
    "y_test0 = X_test0.pop(target_colname).values\n",
    "y_test0.fill(np.nan)\n",
    "r0 = cm.forecast(X_test0, y_test0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_frames' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-30f67564a660>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# OK to copy the smaller test frames\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtarget_colname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_frames\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mXs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtarget_colname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_frames\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_frames' is not defined"
     ]
    }
   ],
   "source": [
    "# Create the full test set, actuals, and query\n",
    "\n",
    "# OK to copy the smaller test frames\n",
    "ys = [ X[target_colname].values for X in test_frames ]\n",
    "Xs = [ X.copy().drop(columns=[target_colname]) for X in test_frames]\n",
    "X_test = pd.concat(Xs)\n",
    "y_test = np.concatenate(ys)\n",
    "y_query = y_test.copy()\n",
    "y_query.fill(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on all chunks\n",
    "y_fcst, X_trans = cm.forecast(X_test,y_test)\n",
    "df_all = align_outputs(y_fcst, X_trans, X_test, y_test)\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mean_squared_error(df_all[target_column_name], df_all['predicted']))\n",
    "print(\"[Test Data] \\nRoot Mean squared error: %.2f\" % rmse)\n",
    "mae = mean_absolute_error(df_all[target_column_name], df_all['predicted'])\n",
    "print('mean_absolute_error score: %.2f' % mae)\n",
    "mape = MAPE(df_all[target_column_name], df_all['predicted'])\n",
    "print('MAPE: %.2f' % mape)\n",
    "smape = SMAPE(df_all[target_column_name], df_all['predicted'])\n",
    "print('SMAPE: %.2f' % smape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technique 3: Target transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is extremely challenging due to the abundance of zero values. Also, the forecasts sometimes venture below zero, which will not make sense to business customers. In those cases, it often helps to transform the target value with something like a log-transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   9.00\n",
       "1   6.00\n",
       "2   6.00\n",
       "3   6.00\n",
       "4   1.00\n",
       "Name: Quantity, dtype: float64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def log_transform(nparr):\n",
    "    return nparr.apply(lambda x: math.log(x+1))\n",
    "    \n",
    "\n",
    "def log_inverse_transform(nparr):\n",
    "    \"\"\"\n",
    "    With zero truncation so not a perfect inverse\n",
    "    \"\"\"\n",
    "    return nparr.apply(lambda y: max(0, math.exp(y) - 1))\n",
    "    \n",
    "# For example, \n",
    "log_inverse_transform(log_transform(y_train[0:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "model_valid_for = indices[idx]\n",
    "X_train = train_frames[idx].copy()\n",
    "X_test = test_frames[idx].copy()\n",
    "\n",
    "# NOTE log transform\n",
    "y_train = log_transform(X_train.pop(target_colname).values)\n",
    "y_test = X_test.pop(target_colname).values\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_config = AutoMLConfig(task='forecasting',\n",
    "                             debug_log='automl-grocery.log',\n",
    "                             primary_metric='normalized_root_mean_squared_error',\n",
    "                             iterations=5,\n",
    "                             X=X_train,\n",
    "                             y=y_train,                             \n",
    "                             n_cross_validations=2,\n",
    "                             enable_ensembling=False,\n",
    "                             path=project_folder,\n",
    "                             verbosity=logging.INFO,    \n",
    "                             **time_series_settings)\n",
    "\n",
    "local_run = experiment.submit(automl_config, show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run, fitted_pipeline = local_run.get_output()\n",
    "fitted_pipeline.steps\n",
    "y_query = y_test.copy()\n",
    "y_query.fill(np.nan)\n",
    "y_fcst, X_trans = fitted_pipeline.forecast(X_test, y_query)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = {'Item' : 'A0CA5B',\n",
    "        'Site' : '1B3B',\n",
    "        'Channel' : '811B' }\n",
    "\n",
    "# TESTME: do we need to plot from the aligned data frame?\n",
    "plot_forecast(X_train, y_train, X_test, y_test, \n",
    "                  inverse_log_transform(y_fcst),       # NOTE inverse transform\n",
    "                  target_colname,\n",
    "                  time_colname,\n",
    "                  filter_dict = filt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caveat 4: A note on pandas indexes\n",
    "\n",
    "One problem that often occurs with dataframes arising from filtering is that *level names* are retained in the index when the values are *removed*. This can create empty groups when grouping by index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Item</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sales\n",
       "Item       \n",
       "A         2\n",
       "B         4"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This problem is pandas version dependent (TODO: which versions?)\n",
    "print(pd.__version__)\n",
    "\n",
    "ex_df = pd.DataFrame({'Item' : ['A', 'A', 'B', 'B', 'C'], \n",
    "                       'Day' : ['Mon', 'Tue', 'Mon', 'Tue', 'Mon'],\n",
    "                       'Sales' : [1, 2, 3, 4, 5]})\n",
    "ex_df.set_index(['Item'], inplace=True)\n",
    "ex_df = ex_df[ex_df['Day'] == 'Tue']  # filter down to Tue, leaving out 'C'\n",
    "# ex_df.index.get_level(level='Item')\n",
    "ex_df.groupby(level='Item').sum()\n",
    "\n",
    "# TODO: make this problem actually show up with the wrong pandas version.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the easiest fix is to rebuild the dataframe from csv\n",
    "ex_df.reset_index().to_csv('temp_ex_df.csv.bz2', index=False)\n",
    "ex_df = pd.read_csv('temp_ex_df.csv.bz2')\n",
    "\n",
    "# or use StringIO if you having multiple data copies in memory is acceptable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This causes hard-to-understand errors, so let's make sure it didn't happen to us, checking ALL the data chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for empty levels\n",
    "\n",
    "def empty_grains(df, grain_colnames):\n",
    "    \"\"\"\"\n",
    "    returns the list of empty grains\n",
    "    \"\"\"\n",
    "    empties = df.groupby(grain_colnames).filter(lambda g: len(g) == 0)\n",
    "    return empties\n",
    "\n",
    "frames_with_empties = [empty_grains(df, grain_colnames).shape[0] > 0 for df in train_frames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "any(frames_with_empties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This is *different* from checking whether some of the training data frames are actually empty. That would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for empty dataframes in train_frames\n",
    "[idx for idx, df in enumerate(train_frames) if len(df)==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions: What is missing here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise does not address a few things. But you are now equipped to build them on your own.\n",
    "* Deployment of composite models\n",
    " * The deployment procedure will need to register all the models in the dictionary.\n",
    " * The indices dictionary will need to be deployed to the docker image in pickled form.\n",
    " * The template script `init` function will load the indices into memory.\n",
    " * The template script `run` function will need lazy loading of the models from the workspace.\n",
    "* Those orphaned short series.\n",
    " * We left a number of series behind when we decided they were \"too short for ML\". That is quite some hubris;\n",
    "   chances are the business owners do need those forecasts. \n",
    "  * We should therefore create another wraparound model which will provide forecasts for them. \n",
    "    High accuracies cannot be expected from such data, and a simple model will do. \n",
    "    In the pattern of the composite model, we could keep a dictionary of retained parameters\n",
    "    of say, an exponential smoothing model, for each grain in the dataset.\n",
    "* Info schedule issues\n",
    " * TODO: write the function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (azure_automl)",
   "language": "python",
   "name": "azure_automl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
