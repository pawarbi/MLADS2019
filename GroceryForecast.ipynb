{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practicalities of AutoML forecasting\n",
    "\n",
    "In this notebook, we will walk through the process of trying to actually push a real world dataset through AutoML. AutoML has expectations of clean data which are rarely encountered with real world data. We will show various techniques of detecting the problems before they crash AutoML, and workarounds that will get you through the day.\n",
    "\n",
    "Eventually, these tips and tricks will become part of the AutoML product, prettied up and made more robust.\n",
    "\n",
    "To begin, let's load some data. Our first dead end occurs in the data loading, and is shown in the companion notebook `GetDataFromSQL`. But for now, _assume_ we have the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "SQLdata = pandas.read_csv('data/tutorial_data.csv.bz2')\n",
    "SQLdata.describe(include = 'all')\n",
    "\n",
    "# depending on the size your machine, you might want to filter down further to get through the notebook\n",
    "# uncomment this filter\n",
    "SQLdata = SQLdata[SQLdata['Site'] == '1B3B']\n",
    "SQLdata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is a friendly 1.2M rows, which is because it only contains the 348 items beginning with 'A'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQLdata['Item'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll establish the time series metadata. These say: we want to forecast the Quantity, along the time axis given by SalesDate, for each combination of Item, Site, and Channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grain_colnames = ['Item', 'Site', 'Channel']\n",
    "time_colname = 'SalesDate'\n",
    "target_colname = 'Quantity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many series do we have?\n",
    "guppy = SQLdata.groupby(grain_colnames)\n",
    "\n",
    "print(\"Rows pulled : \" + str(len(SQLdata)))\n",
    "print(\"Distinct time series : \" + str(len(guppy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's forecast!\n",
    "\n",
    "AutoML in theory does not need much more than a data frame. So let's see how far AutoML gets on this **tough** dataset. Let us not even split into train and test (because we may need to re-work the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = SQLdata.copy()\n",
    "y_train = X_train.pop(target_colname).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin our forecasting journey with the boilerplate AutoML imports and connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import warnings\n",
    "import os\n",
    "# Squash warning messages for cleaner output in the notebook\n",
    "warnings.showwarning = lambda *args, **kwargs: None\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "# choose a name for the experiment in the workspace\n",
    "# 'Experiment' roughly corresponds to a 'problem to solve'\n",
    "experiment_name = 'automl-grocery'\n",
    "# where to write models and other artifacts locally\n",
    "project_folder = './sample_projects/automl-grocery'\n",
    "experiment = Experiment(ws, experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {}\n",
    "output['SDK version'] = azureml.core.VERSION\n",
    "output['Subscription ID'] = ws.subscription_id\n",
    "output['Workspace'] = ws.name\n",
    "output['Resource Group'] = ws.resource_group\n",
    "output['Location'] = ws.location\n",
    "output['Project Directory'] = project_folder\n",
    "output['Run History Name'] = experiment_name\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "outputDf = pd.DataFrame(data = output, index = [''])\n",
    "outputDf.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we specify what our forecasting problem is, put the spec in the config object form, and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_settings = {\n",
    "    'time_column_name': time_colname,\n",
    "    'grain_column_names': grain_colnames,\n",
    "    'drop_column_names': [],\n",
    "    'max_horizon': 31\n",
    "}\n",
    "\n",
    "automl_config = AutoMLConfig(task='forecasting',\n",
    "                             debug_log='automl-grocery.log',\n",
    "                             primary_metric='r2_score',\n",
    "                             iterations=5,\n",
    "                             X=X_train,\n",
    "                             y=y_train,                             \n",
    "                             n_cross_validations=2,\n",
    "                             enable_ensembling=False,\n",
    "                             path=project_folder,\n",
    "                             verbosity=logging.INFO,    \n",
    "                             **time_series_settings)\n",
    "\n",
    "local_run = experiment.submit(automl_config, show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The errors are expected: our dataset has many intermittent series. For example, look at this series, focusing on the `SalesDate` column. Could *you* determine what its natural reporting frequency is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQLdata[(SQLdata['Item'] == 'A0CA5B') & (SQLdata['Site'] == '1B3B') & (SQLdata['Channel'] == '811B')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technique 1: Filling in the implicit zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's help AutoML determine what the periodicity is by making the series explicitly daily. Since we are talking about sales, missing values correspond to zero quantity sold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have a nice helpful function that fills out the data frame with:\n",
    "# * zeros for the target column\n",
    "# * NaNs for the rest of the data\n",
    "# \n",
    "# Let's import it from out time series library in the repo.\n",
    "from tslib.ts_functions import fill_out_with_zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since frequency is hard to infer with intermittent data, we will tell the function explicitly\n",
    "# the frequency is daily\n",
    "complete = fill_out_with_zeros(SQLdata, time_colname, grain_colnames, target_colname, 'D')\n",
    "complete.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the data engorged to 6M rows - all the days with no record now have explicit 0 sales. We call that a \"flat\" dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data comes out with the grain and time in a pandas MultiIndex\n",
    "flat_complete = complete.reset_index()\n",
    "flat_complete.to_csv(\"grocery_flat.csv.bz2\", compression='bz2', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how do I split the data? It's about 5 years of data, 2012-01-01 to 2017-08-31.\n",
    "# It probably does not make sense to do more than one month ahead of daily predictions\n",
    "n_test_periods = 31 # days in August 2017\n",
    "\n",
    "# this will split off the last n VALUES from each grain. If the data has implicit zeros,\n",
    "# that means the test TIME INTERVAL may vary by grain.\n",
    "from tslib.ts_functions import split_last_n_by_grain\n",
    "\n",
    "# Do note the use of the parameter specifying the minimum grain length.\n",
    "# Because of how we pulled the data from SQL, the use is redundant.\n",
    "# But if your whole data fits in memory, you can filter out the short grains\n",
    "# in python, rather than at the source, using the splitting function\n",
    "df_train, df_test = split_last_n_by_grain(complete, \n",
    "                                          n_test_periods, \n",
    "                                          time_colname, grain_colnames, \n",
    "                                          min_grain_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know how many datapoints we have. But do we really know where the train and test sets are situated in time?\n",
    "Let's look at their time extents. For ease of plotting, only two items (but with all their Site and Channel combinations) will be shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "smaller = complete.loc[(list(X_train['Item'].drop_duplicates())[3:5], slice(None), slice(None), slice(None)), :]\n",
    "small_train, small_test = split_last_n_by_grain(smaller, n_test_periods, time_colname, grain_colnames, min_grain_length=100)\n",
    "print(small_train.shape)\n",
    "print(small_test.shape)\n",
    "\n",
    "# this is for checking the time extent of the grains\n",
    "ranges_train = small_train.reset_index().groupby(grain_colnames).agg({'SalesDate': ['min', 'max']})\n",
    "ranges_train.columns = ['begin', 'end']\n",
    "ranges_train_flat = ranges_train.reset_index()\n",
    "\n",
    "ranges_test = small_test.reset_index().groupby(grain_colnames).agg({'SalesDate': ['min', 'max']})\n",
    "ranges_test.columns = ['begin', 'end']\n",
    "ranges_test_flat = ranges_test.reset_index()\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = [12, 4]\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hlines(range(len(ranges_train_flat)),xmin=ranges_train_flat['begin'].values,xmax=ranges_train_flat['end'].values)\n",
    "plt.hlines(range(len(ranges_test_flat)),xmin=ranges_test_flat['begin'].values,xmax=ranges_test_flat['end'].values, colors='r')\n",
    "ytick_tuples = list(zip(ranges_train_flat['Item'],ranges_train_flat['Site'], ranges_train_flat['Channel']))\n",
    "ytick_labels = list(map(lambda t : '-'.join(t), ytick_tuples))\n",
    "plt.yticks(range(len(ranges_test_flat)), ytick_labels)\n",
    "plt.show()\n",
    "\n",
    "# Improvements on this plot could include \n",
    "#    * plot little series instead of horizontal lines, \n",
    "#    * color-separating zero values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should ask ourselves whether the items whose histories do not reach the temporal end of the dataset are worth forecasting. Those items may have been discontinued, or they are still carried by the store but purchased infrequently. Since that is a question only a business owner can answer, let's leave them in for now. But have that little practicality in mind.\n",
    "\n",
    "Let's save the dataset we have in case we need to restart from this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint: flatten for writing and save\n",
    "flat_test = df_test.reset_index()\n",
    "flat_test.to_csv(\"grocery_flat_test.csv.bz2\", compression='bz2', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_train = df_train.reset_index()\n",
    "flat_train.to_csv(\"grocery_flat_trainvalid.csv.bz2\", compression='bz2', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine what X and Y are - now with the zeros\n",
    "X_train = flat_train\n",
    "X_test = flat_test\n",
    "y_train = X_train.pop(target_colname).values\n",
    "y_test = X_test.pop(target_colname).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take some time to finish with 6M rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_config = AutoMLConfig(task='forecasting',\n",
    "                             debug_log='automl-grocery.log',\n",
    "                             primary_metric='r2_score',\n",
    "                             iterations=5,\n",
    "                             X=X_train,   # Here the X_train and y_train have\n",
    "                             y=y_train,   # explicit zeroes                             \n",
    "                             n_cross_validations=2,\n",
    "                             enable_ensembling=False,\n",
    "                             path=project_folder,\n",
    "                             verbosity=logging.INFO,    \n",
    "                             **time_series_settings)\n",
    "\n",
    "local_run = experiment.submit(automl_config, show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we are not testing here because we pretend AutoML had a memory error. The data we are using in the tutorial may not actually be too large as we are only using items beginning with 'A'. But with the full dataset, the AutoML run in the above cell did fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technique 2: Splitting the datasets and composite models\n",
    "\n",
    "## (NOT your usual train/test splitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint: restarting the processing from saved files\n",
    "# A good practice if you work with dataset sizes that crash python kernels\n",
    "grain_colnames = ['Item', 'Site', 'Channel']\n",
    "time_colname = 'SalesDate'\n",
    "target_colname = 'Quantity'\n",
    "\n",
    "import pandas as pd\n",
    "flat_train = pd.read_csv(\"grocery_flat_trainvalid.csv.bz2\", compression='bz2', parse_dates=[time_colname])\n",
    "flat_test = pd.read_csv(\"grocery_flat_test.csv.bz2\", compression='bz2', parse_dates=[time_colname])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We arrived at this point because AutoML had an out-of-memory event. \n",
    "\n",
    "If the data is too large, we need to split the series into multiple chunks to be handled separately. The data amounts in the individual chunks should ideally be close to equal.\n",
    "\n",
    "It is best to split by the value of a specified column. It should be one of the grain columns - that way grains are guaranteed not to split across bins. \n",
    "\n",
    "'Item' seems like a good candidate column. How variable do the Item-based groups get in size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_train.groupby('Item').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the size is quite variable. Randomly assigning each item to a subset (chunk) would likely achieve approximately even chunks if the Items were about equal in data size, but we see the variation spans orders of magnitudes.\n",
    "\n",
    "Therefore, after grouping by column value, then approximately solve the bin packing problem with maximum volume equal to the max number of rows we are willing to have for one model. This will guarantee near-optimal packing.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslib.ts_functions import split_into_chunks_by_size, split_into_chunks_by_groups\n",
    "    \n",
    "max_number_of_rows = 100 * 1000;\n",
    "train_frames, indices = split_into_chunks_by_size(flat_train, 'Item', max_number_of_rows)\n",
    "\n",
    "# split test set using the same assignment of item to chunk (indices)\n",
    "test_frames = split_into_chunks_by_groups(flat_test, 'Item', indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpointing: pickle the split datasets\n",
    "import pickle\n",
    "pickle.dump( (train_frames, test_frames, indices), open('split_datasets.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and load them back after the lengthy procedure\n",
    "import pickle\n",
    "(train_frames, test_frames, indices) = pickle.load(open('split_datasets.pkl', 'rb'))\n",
    "\n",
    "grain_colnames = ['Item', 'Site', 'Channel']\n",
    "time_colname = 'SalesDate'\n",
    "target_colname = 'Quantity'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ensure we got decently sized chunks. Each chunk is a separate dataset that we will feed into AutoML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "model_valid_for = indices[idx]\n",
    "X_train = train_frames[idx].copy()\n",
    "X_test = test_frames[idx].copy()\n",
    "\n",
    "y_train = X_train.pop(target_colname).values\n",
    "y_test = X_test.pop(target_colname).values\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the first chunk has the expected size for the train and test dataset. Let's test whether we messed up the datasets in the process of splitting. Running it through AutoML will run the data checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_settings = {\n",
    "    'time_column_name': time_colname,\n",
    "    'grain_column_names': grain_colnames,\n",
    "    'drop_column_names': [],\n",
    "    'max_horizon': 31\n",
    "}\n",
    "\n",
    "automl_config = AutoMLConfig(task='forecasting',\n",
    "                             debug_log='automl-grocery.log',\n",
    "                             primary_metric='r2_score',\n",
    "                             iterations=5,\n",
    "                             X=X_train,   # set the train to the NEW train set\n",
    "                             y=y_train,   # which is now the first chunk only                      \n",
    "                             n_cross_validations=2,\n",
    "                             enable_ensembling=False,\n",
    "                             path=project_folder,\n",
    "                             verbosity=logging.INFO,    \n",
    "                             **time_series_settings)\n",
    "\n",
    "local_run = experiment.submit(automl_config, show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run, fitted_pipeline = local_run.get_output()\n",
    "fitted_pipeline.steps\n",
    "y_query = y_test.copy()\n",
    "y_query.fill(np.nan)\n",
    "y_fcst, X_trans = fitted_pipeline.forecast(X_test, y_query)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslib.ts_functions import align_outputs\n",
    "df_all = align_outputs(y_fcst, X_trans, X_test, y_test, target_colname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That worked! Now that we were able to get a model, let's import the preferred and customary forecasting metrics and see how well we are doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslib.ts_functions import MAPE, SMAPE, MAE\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(df_all[target_colname], df_all['predicted']))\n",
    "print(\"[Test Data] \\nRoot Mean squared error: %.2f\" % rmse)\n",
    "mae = mean_absolute_error(df_all[target_colname], df_all['predicted'])\n",
    "print('mean_absolute_error score: %.2f' % mae)\n",
    "mape = MAPE(df_all[target_colname], df_all['predicted'])\n",
    "print('MAPE: %.2f' % mape)\n",
    "smape = SMAPE(df_all[target_colname], df_all['predicted'])\n",
    "print('SMAPE: %.2f' % smape)\n",
    "mae = MAE(df_all[target_colname], df_all['predicted'])\n",
    "print('MAE: %.2f' % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MAPE, and even SMAPE metrics can be deceiving when dealing with time series that are often close to zero. They divide by small values and are quite volatile. This makes them a poor fit for such problems. \n",
    "\n",
    "MAE is a better metric for these cases because it avoids division. But be careful because MAE results are unnormalized and comparing them only makes sense in reference to a fixed test set. MAE places weight on series whose absolute value is large. This is often correct in business. Mis-forecasting the Microsoft intern program travel expenses is a lot smaller deal than mis-forecasting the Azure datacenter operations cost. Series trained towards MAE should be of similar magnitude. \n",
    "\n",
    "It is often helpful to stratify the series by the `ceil(log(mean(series)))` and train separate models which allows for more homogeneous training set magnitude-wise and will lead to better performance on smaller-magnitude series.\n",
    "\n",
    "But the best way to get a sense of how well a classifier is performing is to just plot the forecasts vs actuals. The mini-library has some plotting functions for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do a time series plot for our difficult intermittent series\n",
    "from tslib.ts_plot_utils import plot_forecast, jupyter_matplotlib_magic\n",
    "\n",
    "filt = {'Item' : list(X_test['Item'].drop_duplicates())[0],\n",
    "        'Site' : '1B3B',\n",
    "        'Channel' : '811B' }\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "# jupyter_matplotlib_magic()\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = [12, 8]\n",
    "\n",
    "plot_train, plot_test = \\\n",
    "   plot_forecast(X_train, y_train, \n",
    "                  df_all.copy().drop(columns=target_colname), # X_test, aligned\n",
    "                  df_all.copy().pop(target_colname).values,   # y_test, aligned\n",
    "                  df_all.copy().pop('predicted').values,      # y_fcst, aligned\n",
    "                  target_colname, time_colname, grain_colnames,\n",
    "                  filter_dict = filt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks less than optimal. This may be caused by model optimizing towards the many near-zero series, an argument for building multiple models stratified by volume. Let's do a scatter plot of actual vs predicted for the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "test_pred = plt.scatter(df_all[target_colname], df_all['predicted'], color='b')\n",
    "test_actl = plt.scatter(df_all[target_colname], df_all[target_colname], color='g')\n",
    "plt.legend((test_pred, test_actl), ('prediction', 'actual'), loc='upper left', fontsize=8)\n",
    "plt.xlabel('actual')\n",
    "plt.ylabel('predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This underprediction of higher demands would make a good case for stratification or adding lags, but let's complete the exercise and return to the problem later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting the models back together\n",
    "\n",
    "We split the data into small enough chunks that we can learn a model from each. But this is mildly inconvenient - we now have a collection of models and each of them can only process a subset of the data. We need to keep track of which models serves which `Item`s. To do this without writing awkward code on your side, we offer the general technique of *composite models*.\n",
    "\n",
    "In the forecasting case, we usually have a natural splitting column in the grain. It would be extraordinary to have a single time series (no grain) big enough to require this treatment.\n",
    "\n",
    "In the more general case of classification or regression, no good categorical splitting column may exist in the data. You can still split the data, and all the models you learn will be \"universal\" - capable of classifying any instance. You will lose some predictive power, because you learned from smaller datasets. But you can get it back by running the example through `predict()` of *all* the models and ensembling the predictions (just take a simple majority vote or average).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompositeModel:    \n",
    "    \n",
    "    def __init__(self, split_column, target_column):\n",
    "        self._split_column = split_column\n",
    "        self._target_column = target_column\n",
    "        self._item_run_map = dict()\n",
    "        self._item_model_map = dict()\n",
    "        self._model_impls = dict()\n",
    "        self._indices = dict()\n",
    "    \n",
    "\n",
    "    # TODO: redo this so that this class splits its own training data\n",
    "    # on the splitcolumn, ideally in sequence, rather than materializing\n",
    "    # the whole dataset again in memory as a sequence of chunks. \n",
    "    \n",
    "    def fit(self, train_frames, indices):    \n",
    "        \n",
    "        self._indices = indices\n",
    "            \n",
    "        # TODO: this is hard-wired and overwrites outer scope.\n",
    "        # Consume these params in constructor instead.\n",
    "        time_series_settings = {\n",
    "            'time_column_name': time_colname,\n",
    "            'grain_column_names': grain_colnames,\n",
    "            'drop_column_names': [],\n",
    "            'max_horizon': 31\n",
    "        }\n",
    "        \n",
    "        for idx, Xy in enumerate(train_frames):\n",
    "            \n",
    "            if len(Xy) == 0:\n",
    "                print('Warning: found a zero-length frame at index ' + str(idx))\n",
    "                continue\n",
    "            \n",
    "            X_train = Xy.copy()            \n",
    "            y_train = X_train.pop(self._target_column).values                        \n",
    "          \n",
    "            automl_config = AutoMLConfig(task='forecasting',\n",
    "                                debug_log='automl-grocery.log',\n",
    "                                primary_metric='r2_score',\n",
    "                                iterations=5,\n",
    "                                X=X_train,\n",
    "                                y=y_train,                             \n",
    "                                n_cross_validations=3,\n",
    "                                enable_ensembling=False,\n",
    "                                path=project_folder,\n",
    "                                verbosity=logging.INFO,    \n",
    "                                **time_series_settings)\n",
    "        \n",
    "            # get the model and metadata\n",
    "            local_run = experiment.submit(automl_config, show_output=True) # Parent run \n",
    "            best_run, fitted_pipeline = local_run.get_output()             # Favorite child\n",
    "            model_id = best_run.id\n",
    "            print('Learned model ' + str(model_id))  # this is not working - needs a different ID\n",
    "        \n",
    "            # record the model for item\n",
    "            self._model_impls[model_id] = fitted_pipeline\n",
    "            for idx, item in enumerate(self._indices[idx]):\n",
    "                self._item_model_map[item] = model_id\n",
    "                self._item_run_map[item] = best_run.id\n",
    "                \n",
    "                \n",
    "    def forecast(self, X_test, y_test):\n",
    "        \n",
    "        # split X and y together by splitcolumn\n",
    "        X_copy = X_test.copy()\n",
    "        X_copy['__automl_target_column'] = y_test\n",
    "        chunks = split_into_chunks_by_groups(X_copy, self._split_column, self._indices)\n",
    "        \n",
    "        ys = []\n",
    "        X_transes = []\n",
    "        for chunk in chunks:\n",
    "            \n",
    "            # skip potentially empty splits\n",
    "            if len(chunk) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Look up the right model. It should be the same model \n",
    "            # for the whole chunk by construction\n",
    "            item = chunk.loc[chunk.index[0], self._split_column]\n",
    "            modelid = self._item_model_map[item]\n",
    "            print('Using model ' + str(modelid))\n",
    "            model = self._model_impls[modelid]\n",
    "            \n",
    "            paranoid = True\n",
    "            if paranoid:\n",
    "                for item2 in pd.unique(chunk[self._split_column]):\n",
    "                    assert(\n",
    "                           (item2 in self._item_model_map.keys()) and \n",
    "                           (self._item_model_map[item2] == modelid),\n",
    "                           'Item ' + str(item2) + ' is not mapped to the same model ' + str(modelid) + ' as ' + str(item)\n",
    "                    )\n",
    "                           \n",
    "            y_chunk = chunk.pop('__automl_target_column').values\n",
    "            y_pred, X_trans = model.forecast(chunk, y_chunk)\n",
    "            ys.append(y_pred)\n",
    "            X_transes.append(X_trans)\n",
    "            \n",
    "        return np.concatenate(ys), pd.concat(X_transes)                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cm = CompositeModel(split_column = 'Item', target_column = 'Quantity')\n",
    "cm.fit(train_frames, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the forecasts on one test dataframe\n",
    "X_test0 = test_frames[0].copy()\n",
    "y_test0 = X_test0.pop(target_colname).values\n",
    "y_test0.fill(np.nan)\n",
    "r0, X_trans0 = cm.forecast(X_test0, y_test0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the full test set, actuals, and query\n",
    "\n",
    "# OK to copy the smaller test frames\n",
    "ys = [ X[target_colname].values for X in test_frames ]\n",
    "Xs = [ X.copy().drop(columns=[target_colname]) for X in test_frames]\n",
    "X_test = pd.concat(Xs)\n",
    "y_test = np.concatenate(ys)\n",
    "y_query = y_test.copy()\n",
    "y_query.fill(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on all chunks\n",
    "y_fcst, X_trans = cm.forecast(X_test,y_query)\n",
    "df_all = align_outputs(y_fcst, X_trans, X_test, y_test, target_colname)\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mean_squared_error(df_all[target_colname], df_all['predicted']))\n",
    "print(\"[Test Data] \\nRoot Mean squared error: %.2f\" % rmse)\n",
    "mae = mean_absolute_error(df_all[target_colname], df_all['predicted'])\n",
    "print('mean_absolute_error score: %.2f' % mae)\n",
    "mape = MAPE(df_all[target_colname], df_all['predicted'])\n",
    "print('MAPE: %.2f' % mape)\n",
    "smape = SMAPE(df_all[target_colname], df_all['predicted'])\n",
    "print('SMAPE: %.2f' % smape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "test_pred = plt.scatter(df_all[target_colname], df_all['predicted'], color='b')\n",
    "test_actl = plt.scatter(df_all[target_colname], df_all[target_colname], color='g')\n",
    "plt.legend((test_pred, test_actl), ('prediction', 'actual'), loc='upper left', fontsize=8)\n",
    "plt.xlabel('actual')\n",
    "plt.ylabel('predicted')\n",
    "plt.xlim([0,75])\n",
    "plt.ylim([0,75])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technique 3: Target transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is extremely challenging due to the abundance of zero values. Also, the forecasts sometimes venture below zero, which will not make sense to business customers. In those cases, it often helps to transform the target value with something like a log-transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def log_transform(nparr):\n",
    "    return np.vectorize(lambda x: math.log(x+1))(nparr)\n",
    "    \n",
    "\n",
    "def inverse_log_transform(nparr):\n",
    "    \"\"\"\n",
    "    With zero truncation so not a perfect inverse\n",
    "    \"\"\"\n",
    "    return np.vectorize(lambda y: max(0, math.exp(y) - 1))(nparr)\n",
    "    \n",
    "# For example, \n",
    "inverse_log_transform(log_transform(y_train[0:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "model_valid_for = indices[idx]\n",
    "X_train = train_frames[idx].copy()\n",
    "X_test = test_frames[idx].copy()\n",
    "\n",
    "# NOTE log transform\n",
    "y_train = log_transform(X_train.pop(target_colname).values)\n",
    "y_test = X_test.pop(target_colname).values\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_config = AutoMLConfig(task='forecasting',\n",
    "                             debug_log='automl-grocery.log',\n",
    "                             primary_metric='r2_score',\n",
    "                             iterations=5,\n",
    "                             X=X_train,\n",
    "                             y=y_train,                             \n",
    "                             n_cross_validations=2,\n",
    "                             enable_ensembling=False,\n",
    "                             path=project_folder,\n",
    "                             verbosity=logging.INFO,    \n",
    "                             **time_series_settings)\n",
    "\n",
    "local_run = experiment.submit(automl_config, show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run, fitted_pipeline = local_run.get_output()\n",
    "fitted_pipeline.steps\n",
    "y_query = y_test.copy()\n",
    "y_query.fill(np.nan)\n",
    "y_fcst, X_trans = fitted_pipeline.forecast(X_test, y_query)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = align_outputs(inverse_log_transform(y_fcst), # convert back!\n",
    "                       X_trans, X_test, y_test, target_colname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = {'Item' : list(X_test['Item'])[0],\n",
    "        'Site' : '1B3B',\n",
    "        'Channel' : '811B' }\n",
    "\n",
    "# plot_forecast does its own internal join\n",
    "plot_train, plot_test = plot_forecast(X_train, \n",
    "                  inverse_log_transform(y_train),      # NOTE inverse transform\n",
    "                  X_test, y_test,                      # we did not log-transform the test\n",
    "                  inverse_log_transform(y_fcst),       # NOTE inverse transform\n",
    "                  target_colname, time_colname, grain_colnames,\n",
    "                  filter_dict = filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "test_pred = plt.scatter(df_all[target_colname], df_all['predicted'], color='b')\n",
    "test_actl = plt.scatter(df_all[target_colname], df_all[target_colname], color='g')\n",
    "plt.legend((test_pred, test_actl), ('prediction', 'actual'), loc='upper left', fontsize=8)\n",
    "plt.xlabel('actual')\n",
    "plt.ylabel('predicted')\n",
    "plt.xlim([0, 75])\n",
    "plt.ylim([0, 75])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mean_squared_error(df_all[target_colname], df_all['predicted']))\n",
    "print(\"[Test Data] \\nRoot Mean squared error: %.2f\" % rmse)\n",
    "mae = mean_absolute_error(df_all[target_colname], df_all['predicted'])\n",
    "print('mean_absolute_error score: %.2f' % mae)\n",
    "mape = MAPE(df_all[target_colname], df_all['predicted'])\n",
    "print('MAPE: %.2f' % mape)\n",
    "smape = SMAPE(df_all[target_colname], df_all['predicted'])\n",
    "print('SMAPE: %.2f' % smape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target transform did buy us a strictly positive prediction and a slight improvement in the APEs. It is difficult to do much better because this dataset is strongly price dependent and the price variable has been anonymized away."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caveat 4: A note on pandas categorical levels\n",
    "\n",
    "One problem that often occurs with dataframes arising from filtering is that *level names* are retained in the index when the values are *removed*. This can create empty groups when grouping by index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This problem is pandas version dependent \n",
    "print(pd.__version__)\n",
    "\n",
    "ex_df = pd.DataFrame({'Item' : ['A', 'A', 'B', 'B', 'C'], \n",
    "                       'Day' : ['Mon', 'Tue', 'Mon', 'Tue', 'Mon'],\n",
    "                       'Sales' : [1, 2, 3, 4, 5]})\n",
    "ex_df['Item'] = ex_df['Item'].astype('category')\n",
    "ex_df = ex_df[ex_df['Day'] == 'Tue']  # filter down to Tue, leaving out 'C'\n",
    "ex_df.groupby(['Item']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the zero for item 'C'. This comes from an empty group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the easiest fix is to rebuild the dataframe from csv\n",
    "ex_df.reset_index().to_csv('temp_ex_df.csv', index=False)\n",
    "ex_df2 = pd.read_csv('temp_ex_df.csv')\n",
    "\n",
    "# or use StringIO if you having multiple data copies in memory is acceptable\n",
    "ex_df2.groupby(['Item']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The empty groups cause hard-to-understand errors inside AutoML. Let's make sure it didn't happen to us, checking ALL the data chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for empty levels\n",
    "\n",
    "def empty_grains(df, grain_colnames):\n",
    "    \"\"\"\"\n",
    "    returns the list of empty grains\n",
    "    \"\"\"\n",
    "    empties = df.groupby(grain_colnames).filter(lambda g: len(g) == 0)\n",
    "    return empties\n",
    "\n",
    "frames_with_empties = [empty_grains(df, grain_colnames).shape[0] > 0 for df in train_frames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "any(frames_with_empties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This is *different* from checking whether some of the training data frames are actually empty. That would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for empty dataframes in train_frames\n",
    "[idx for idx, df in enumerate(train_frames) if len(df)==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions: What is missing here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise does not address a few things. But you are now equipped to build them on your own.\n",
    "* Deployment of composite models\n",
    " * The deployment procedure will need to register all the models in the dictionary.\n",
    " * The indices dictionary will need to be deployed to the docker image in pickled form.\n",
    " * The template script `init` function will load the indices into memory.\n",
    " * The template script `run` function will need lazy loading of the models from the workspace.\n",
    "* Those orphaned short series.\n",
    " * At the very beginning, we left a number of series behind when we decided they were \"too short for ML\". \n",
    "   That is quite some hubris; chances are the business owners do need those forecasts. \n",
    "  * We should therefore create another wraparound model which will provide forecasts for them. \n",
    "    High accuracies cannot be expected from such data, and a simple model will do. \n",
    "    In the pattern of the composite model, we could keep a dictionary of retained parameters\n",
    "    of say, an exponential smoothing model, for each grain in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (azure_automl)",
   "language": "python",
   "name": "azure_automl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
